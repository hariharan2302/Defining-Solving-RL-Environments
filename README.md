# ğŸ¯ Reinforcement Learning Environments: Q-Learning & SARSA Implementation

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29.1-green.svg)](https://gymnasium.farama.org/)
[![License](https://img.shields.io/badge/License-Educational-orange.svg)](#license)

A comprehensive implementation of **Q-Learning** and **SARSA** algorithms in both deterministic and stochastic environments, featuring custom GridWorld environments and a practical stock trading application using reinforcement learning techniques.

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Features](#-features)
- [ğŸ—ï¸ Project Structure](#ï¸-project-structure)
- [ğŸ® Environments](#-environments)
- [ğŸ§  Algorithms](#-algorithms)
- [ğŸ“Š Results & Analysis](#-results--analysis)
- [ğŸš€ Getting Started](#-getting-started)
- [ğŸ“ˆ Stock Trading Application](#-stock-trading-application)
- [âš™ï¸ Hyperparameters](#ï¸-hyperparameters)
- [ğŸ”¬ Experimental Setup](#-experimental-setup)
- [ğŸ“¸ Visual Results](#-visual-results)
- [ğŸ›¡ï¸ Safety Measures](#ï¸-safety-measures)
- [ğŸ‘¨â€ğŸ’» Author](#-author)
- [ğŸ“š Course Information](#-course-information)
- [ğŸ“„ License](#-license)

## ğŸŒŸ Features

### ğŸ¯ **GridWorld Environments**
- **Deterministic Environment**: Predictable state transitions with guaranteed outcomes
- **Stochastic Environment**: Probabilistic transitions with uncertainty modeling
- **Multi-reward System**: 
  - ğŸŸ¢ **Goal Reward**: +10 (Terminal state)
  - ğŸ”µ **Positive Rewards**: +1 (Intermediate rewards)
  - ğŸ”´ **Negative Rewards**: -1 (Penalty states)
- **State Space**: 36 discrete states (6Ã—6 grid)
- **Action Space**: 4 discrete actions (â†‘ UP, â†“ DOWN, â†’ RIGHT, â† LEFT)

### ğŸ§  **Learning Algorithms**
- **Q-Learning**: Off-policy temporal difference learning
- **SARSA**: On-policy temporal difference learning
- **Comparative Analysis**: Performance evaluation between algorithms
- **Hyperparameter Optimization**: Systematic tuning of learning parameters

### ğŸ“ˆ **Stock Trading Environment**
- **Real Market Data**: NVIDIA (NVDA) stock data from 2021-2024
- **RL-based Trading**: Intelligent buy/sell/hold decisions
- **Performance Tracking**: Comprehensive reward and profit analysis
- **Risk Management**: Built-in safety measures for trading decisions

## ğŸ—ï¸ Project Structure

```
ğŸ“¦ Defining-Solving-RL-Environments/
â”œâ”€â”€ ğŸ““ PA1_hvenkatr (2).ipynb          # Main implementation notebook
â”œâ”€â”€ ğŸ“Š NVDA.csv                        # Stock market data
â”œâ”€â”€ ğŸ–¼ï¸ images/                         # Environment visualizations
â”‚   â”œâ”€â”€ RL_proj_agent.jpg             # Agent representation
â”‚   â”œâ”€â”€ RL_proj_env.jpg               # Environment overview
â”‚   â”œâ”€â”€ RL_proj_goal.jpg              # Goal state visualization
â”‚   â”œâ”€â”€ RL_proj_obs1.jpg              # Observation examples
â”‚   â”œâ”€â”€ RL_proj_obs2.jpg
â”‚   â””â”€â”€ RL_proj_obs3(moving).jpg
â”œâ”€â”€ ğŸ’¾ Saved Models/
â”‚   â”œâ”€â”€ Deterministic_q_table.pkl     # Q-Learning deterministic model
â”‚   â”œâ”€â”€ Stochastic_q_table.pkl        # Q-Learning stochastic model
â”‚   â”œâ”€â”€ SARSA_deterministic_q_table.pkl # SARSA deterministic model
â”‚   â”œâ”€â”€ Stochastic_SARSA_q_table.pkl  # SARSA stochastic model
â”‚   â””â”€â”€ stock_Q_table.pkl             # Stock trading Q-table
â”œâ”€â”€ ğŸ“„ README.md                       # Project documentation
â””â”€â”€ ğŸ“œ LICENSE                         # License file
```

## ğŸ® Environments

### ğŸ¯ **GridWorld Environment Specifications**

| Component | Deterministic | Stochastic |
|-----------|---------------|------------|
| **Grid Size** | 6Ã—6 | 6Ã—6 |
| **States** | 36 discrete | 36 discrete |
| **Actions** | 4 (â†‘â†“â†â†’) | 4 (â†‘â†“â†â†’) |
| **Transition** | 100% success | 80% intended, 20% random |
| **Start State** | (0,0) | (0,0) |
| **Goal State** | (5,5) | (5,5) |
| **Reward States** | Fixed positions | Fixed positions |

### ğŸ¨ **Environment Layout**

```
ğŸŸ¨ = Start (0,0)    ğŸŸ¢ = Goal (5,5)    ğŸ”µ = +1 Reward    ğŸ”´ = -1 Penalty
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¨  â”‚     â”‚     â”‚     â”‚     â”‚ ğŸ”´  â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚ ğŸ”µ  â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚ ğŸ”µ  â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”´  â”‚     â”‚     â”‚     â”‚     â”‚ ğŸŸ¢  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

## ğŸ§  Algorithms

### ğŸ¯ **Q-Learning Algorithm**
```python
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
                              a'
```
- **Type**: Off-policy
- **Update**: Uses maximum future Q-value
- **Exploration**: Îµ-greedy policy
- **Convergence**: Guaranteed under certain conditions

### ğŸ¯ **SARSA Algorithm**
```python
Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
```
- **Type**: On-policy
- **Update**: Uses actual next action Q-value
- **Exploration**: Îµ-greedy policy
- **Behavior**: More conservative than Q-Learning

## ğŸ“Š Results & Analysis

### ğŸ† **Performance Comparison**

| Algorithm | Environment | Episodes | Final Reward | Convergence |
|-----------|-------------|----------|--------------|-------------|
| Q-Learning | Deterministic | 5000 | ~8.5 | âœ… Stable |
| Q-Learning | Stochastic | 5000 | ~7.2 | âœ… Stable |
| SARSA | Deterministic | 5000 | ~8.3 | âœ… Stable |
| SARSA | Stochastic | 5000 | ~7.8 | âœ… More Stable |

### ğŸ“ˆ **Key Findings**

1. **ğŸ¯ Deterministic Environment**:
   - Both algorithms achieve similar performance
   - Q-Learning slightly faster convergence
   - SARSA more consistent learning curve

2. **ğŸ² Stochastic Environment**:
   - SARSA demonstrates superior stability
   - Q-Learning shows higher variance
   - SARSA better handles uncertainty

3. **ğŸ“Š Hyperparameter Impact**:
   - **Î³ = 0.9**: Optimal balance between immediate and future rewards
   - **Î± = 0.1**: Stable learning rate
   - **Îµ-decay**: Essential for exploration-exploitation balance

## ğŸš€ Getting Started

### ğŸ“‹ **Prerequisites**

```bash
pip install gymnasium matplotlib numpy pandas pickle
```

### ğŸƒâ€â™‚ï¸ **Quick Start**

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/Defining-Solving-RL-Environments.git
cd Defining-Solving-RL-Environments
```

2. **Open the Jupyter notebook**:
```bash
jupyter notebook "PA1_hvenkatr (2).ipynb"
```

3. **Run the experiments**:
   - Execute cells sequentially
   - Observe training progress
   - Analyze results and visualizations

### ğŸ® **Running Experiments**

```python
# Deterministic Environment
env = Deterministic_Environment()
Q, rewards, epsilon_values = Q_logic_d(env, episodes=5000, gamma=0.9)

# Stochastic Environment  
env = Stochastic_Environment()
Q, rewards, epsilon_values = Q_logic_s(env, episodes=5000, gamma=0.9)
```

## ğŸ“ˆ Stock Trading Application

### ğŸ’¹ **Trading Environment Features**

- **ğŸ“Š Real Data**: NVIDIA stock prices (2021-2024)
- **ğŸ¯ Actions**: Buy (0), Sell (1), Hold (2)
- **ğŸ’° Rewards**: Based on profit/loss from trading decisions
- **ğŸ“ˆ State Space**: Price movements and technical indicators
- **ğŸ›¡ï¸ Risk Management**: Position limits and stop-loss mechanisms

### ğŸ“Š **Trading Performance**

| Metric | Value |
|--------|-------|
| **Training Episodes** | 1000 |
| **Average Reward** | Positive trend |
| **Convergence** | âœ… Achieved |
| **Strategy** | Buy-and-hold outperformed |

## âš™ï¸ Hyperparameters

### ğŸ›ï¸ **Optimal Configuration**

| Parameter | Symbol | Value | Description |
|-----------|--------|-------|-------------|
| **Learning Rate** | Î± | 0.1 | Step size for Q-value updates |
| **Discount Factor** | Î³ | 0.9 | Future reward importance |
| **Exploration Rate** | Îµ | 1.0 â†’ 0.01 | Exploration vs exploitation |
| **Episodes** | N | 5000 | Training iterations |
| **Max Steps** | T | 100 | Maximum steps per episode |

### ğŸ“Š **Hyperparameter Sensitivity Analysis**

```
Î³ = 0.3: Short-term focus, faster but suboptimal
Î³ = 0.5: Balanced approach, moderate performance  
Î³ = 0.7: Long-term focus, better final performance
Î³ = 0.9: Optimal balance, best overall results â­
```

## ğŸ”¬ Experimental Setup

### ğŸ§ª **Testing Protocol**

1. **ğŸ¯ Environment Testing**:
   - Deterministic vs Stochastic comparison
   - Multiple random seeds for reproducibility
   - Statistical significance testing

2. **ğŸ“Š Algorithm Comparison**:
   - Q-Learning vs SARSA performance
   - Convergence rate analysis
   - Stability measurements

3. **âš™ï¸ Hyperparameter Tuning**:
   - Grid search over parameter space
   - Cross-validation for robustness
   - Performance metric optimization

## ğŸ“¸ Visual Results

The `images/` folder contains comprehensive visualizations:

- **ğŸ® Environment Layout**: Grid world structure and reward positions
- **ğŸ¤– Agent Behavior**: Movement patterns and decision making
- **ğŸ¯ Goal Achievement**: Path optimization and success rates
- **ğŸ“Š Learning Progress**: Training curves and convergence analysis

## ğŸ›¡ï¸ Safety Measures

### ğŸ”’ **Robustness Features**

- **ğŸ¯ Bounded Actions**: Prevents invalid moves outside grid
- **âœ… State Validation**: Ensures valid state transitions
- **ğŸ“Š Reward Clipping**: Prevents extreme reward values
- **ğŸš§ Boundary Checks**: Grid boundary enforcement
- **ğŸ”„ Episode Limits**: Prevents infinite loops
- **ğŸ“ˆ Convergence Monitoring**: Early stopping for stability

### âš ï¸ **Error Handling**

```python
# Example safety implementation
def safe_action(self, action):
    new_pos = np.clip(self.agent_pos + self.action_effects[action], 0, 5)
    return new_pos
```

## ğŸ‘¨â€ğŸ’» Author

**Hariharan Venkatraman**
- ğŸ“ Graduate Student in Computer Science
- ğŸ« University at Buffalo
- ğŸ“§ Contact: [hvenkatr@buffalo.edu]
- ğŸ”¬ Research Focus: Reinforcement Learning & AI

## ğŸ“š Course Information

- **ğŸ“– Course**: CSE 446/546 - Machine Learning
- **ğŸ« Institution**: University at Buffalo, SUNY
- **ğŸ“… Semester**: Spring 2024
- **ğŸ‘¨â€ğŸ« Focus**: Reinforcement Learning Fundamentals
- **ğŸ¯ Objective**: Practical implementation of RL algorithms

## ğŸ“„ License

This project is for educational purposes as part of CSE 446/546 coursework.

---

<div align="center">

### ğŸŒŸ **Star this repository if you found it helpful!** ğŸŒŸ

**Made with â¤ï¸ for the Reinforcement Learning Community**

</div>